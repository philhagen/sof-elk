# SOF-ELK® Configuration File
# Created by Christophe Vandeplas <christophe@vandeplas.com>
# Please check https://github.com/cvandeplas/ELK-forensics for more information.
# Changes made by Mark Hallman 2019-03-20
# Changes made by Lewes Technology Consulting, LLC for the SOF-ELK® platform

# This file contains transforms and enrichments to be applied in postprocessing

filter {
  if [type] == "plaso" {
    csv {
       separator => ","
       quote_char => "ª"       # workaround: don't use a quote character as " gives issues if the field contains a "
       columns => ["date","time","timezone","macb","datasource","datasourcetype","eventtype","user","host","short","desc","version","filename","inode","notes","format","extra"]
    }
    if [date] == "date" {
       drop {}  # drop the first line that contains the column names
    }

    # assemble a string containing "<%date%> <%time%> <%timezone%>" go be used with the date{} filter
    mutate { merge => [ "date", "time" ] }       # merge and join need to be in separate mutates
    mutate { merge => [ "date", "timezone" ] }   # merge and join need to be in separate mutates
    mutate { join => [ "date", " " ] }           # merge and join need to be in separate mutates

    date {
      match => [ "date", "MM/dd/YYYY HH:mm:ss z", "MM/dd/YYYY HH:mm:ss ZZZ" ]
    }

# PJH TODO: Do we need to unify this with stdinfo.* fields?
# PJH TODO: Holy crap... should we just alter KAPE to use one record per time value?  how does that affect $FILENAME?
#           this is an interesting option - it would require using the clone{} filter
    # extract macb info
    if ("M" in [macb]) { mutate { add_tag => [ "modified" ] } }
    if ("A" in [macb]) { mutate { add_tag => [ "accessed" ] } }
    if ("C" in [macb]) { mutate { add_tag => [ "changed" ] } }
    if ("B" in [macb]) { mutate { add_tag => [ "birth" ] } }

    # extract data from the "desc" field based on the respective datasource value
    # Extract filenames
    if [datasource] == "FILE" or [datasource] == "META" {
      grok {
        break_on_match => false
        match => [
          "desc", "(:(?<path>/.*?))?$",
          "path", "(?<filename>[^/]+?)?$"
        ]
      }

    # Extract urls
    # PJH: This does not seem to hit on the sample data used during development
    } else if [datasource] == "WEBHIST" {
      grok {
        match => [
          "desc", "Location: (?<url>.*?)[ $]"
        ]
      }

    # extract event log data fields
    } else if [datasource] == "EVT" and [datasourcetype] == "WinEVTX" {
      grok {
        patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
        match => [ "desc",  "\[%{POSINT:event_id}.*\] Source Name: %{DATA:provider} Strings: \[%{DATA:payload}\] Computer Name: %{HOSTNAME:computer} Record Number: %{POSINT:record_number} Event Level: %{POSINT:level}" ]
        tag_on_failure => [ "_grokparsefailure_l2t01" ]
      }

    # extract prefetch data fields
    } else if [datasource] == "LOG" and [datasourcetype] == "WinPrefetch" {
      grok {
        patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
        match => [ "desc", "Prefetch \[%{DATA:filename}\] was executed - run count %{POSINT:run_count} path: %{DATA:path} hash: %{WORD:prefetch_hash} volume: %{POSINT:volume_number} \[serial number: %{DATA:volume_serial}  device path: %{DATA:device_path}\]" ]
        tag_on_failure => [ "_grokeparsefail_l2t02" ]
      }
    }

    mutate {
      convert => [
        "inode", "integer",
        "version", "integer"
      ]
      remove_field => [
        "message",
        "short",
        "date",
        "time",
        "timezone"
      ]
    }
  }
}
