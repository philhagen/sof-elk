# SOF-ELK® Configuration File
# Created by Christophe Vandeplas <christophe@vandeplas.com>
# Please check https://github.com/cvandeplas/ELK-forensics for more information.
# Changes made by Mark Hallman 2019-03-20
# (C)2024 changes made by Lewes Technology Consulting, LLC for the SOF-ELK® platform

# This file contains transforms and enrichments to be applied in postprocessing

filter {
  if [labels][type] == "plaso" {
    csv {
       columns => [ "date", "time", "timezone", "macb", "datasource", "datasourcetype", "eventtype", "username", "host", "short", "desc", "version", "filename", "inode", "notes", "format", "extra" ]
       quote_char => "\x00"
       target => "csv"
    }
    if [csv][date] == "date" {
       drop {}  # drop the first line that contains the column names
    }

    # assemble a string containing "<%date%> <%time%> <%timezone%>" go be used with the date{} filter
    mutate {
      replace => {
        "[csv][date]" => "%{[csv][date]} %{[csv][time]} %{[csv][timezone]}"
      }
    }

    date {
      match => [ "[csv][date]", "MM/dd/YYYY HH:mm:ss z", "MM/dd/YYYY HH:mm:ss ZZZ" ]
    }

    # remove empty fields to prevent odd behavior downstream
    if [csv][username] == "-" {
      mutate {
        remove_field => [ "[csv][username]" ]
      }
    }
    if [csv][inode] == "-" {
      mutate {
        remove_field => [ "[csv][inode]" ]
      }
    }
    if [csv][notes] == "-" {
      mutate {
        remove_field => [ "[csv][notes]" ]
      }
    }

    # extract macb info and create appropriate fields
    if ("M" in [csv][macb]) { mutate { copy => { "@timestamp" => "[file][stdinfo][mtime]" } } }
    if ("A" in [csv][macb]) { mutate { copy => { "@timestamp" => "[file][stdinfo][accessed]" } } }
    if ("C" in [csv][macb]) { mutate { copy => { "@timestamp" => "[file][stdinfo][ctime]" } } }
    if ("B" in [csv][macb]) { mutate { copy => { "@timestamp" => "[file][stdinfo][created]" } } }

    # extract data from the "desc" field based on the respective datasource value
    # Extract filenames
    if [csv][datasource] == "FILE" or [csv][datasource] == "META" {
      grok {
        break_on_match => false
        match => { "desc", [
          "(:(?<path>/.*?))?$"
        ] }
        tag_on_failure => [ "_grokparsefail_6601-01"]
      }
      grok {
        break_on_match => false
        match => { "path", [
          "(?<filename>[^/]+?)?$"
        ] }
        tag_on_failure => [ "_grokparsefail_6601-02"]
      }

    # Extract urls
    # PJH: This does not seem to hit on the sample data used during development
    } else if [csv][datasource] == "WEBHIST" {
      grok {
        match => { "desc", [
          "Location: (?<url>.*?)[ $]"
        ] }
        tag_on_failure => [ "_grokparsefail_6601-03"]
      }

    # extract event log data fields
    } else if [csv][datasource] == "EVT" and [csv][datasourcetype] == "WinEVTX" {
      grok {
        patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
        break_on_match => false
        match => { "[csv][desc]" => [
          "^\[%{POSINT:[winlog][event_id]}%{DATA}?.*\]",
          "Provider identifier: %{GUID:[winlog][provider_guid]}",
          "Source Name: %{DATA:[winlog][provider_name]} ",
          # "Strings: \[%{DATA:payload}\]",
          "Computer Name: %{HOSTNAME:[winlog][computer_name]}",
          "Record Number: %{POSINT:[winlog][plaso][record_number]}",
          "Event Level: %{POSINT:[winlog][event_data][IntegrityLevel]}"
          ]
        }
        tag_on_failure => [ "_grokparsefail_6601-04"]
      }

      mutate {
        replace => {
          "[sof-elk][base_index]" => "evtxlogs"
        }
        rename => {
          "[csv][datasource]" => "[plaso][datasource]"
          "[csv][datasourcetype]" => "[plaso][datasourcetype]"
          "[csv][eventtype]" => "[plaso][eventtype]"
          "[csv][filename]" => "[file][source_file]"
        }
      }
      # the xml_string may have important semicolons.  Isolate it for handling,
      # then remove it before we run csv against the remainder
      if [csv][extra] {
        if [csv][extra] =~ "xml_string: " {
          grok {
            patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
            match => { "[csv][extra]" => [
              "xml_string: %{XMLEVENTSTRING:[csv_extra][xml_string]}"
              ]
            }
            tag_on_failure => [ "_grokparsefail_6601-05"]
          }
          if "[csv_extra][xml_string]" {
            mutate {
              gsub => [
                "[csv][extra]", "xml_string: <Event.*/Event>-?", ""
              ]
            }
          }
        }

        kv {
          source => "[csv][extra]"
          target => "[@metadata][kvextra]"
          field_split_pattern => "; "
          value_split_pattern => ": "
        }

        mutate {
          rename => {
            "[@metadata][kvextra][user_sid]" => "[winlog][user][identifier]"
            "[@metadata][kvextra][recovered]" => "[winlog][recovered]"
           # "[@metadata][kvextra][xxx]" => "[xxx]"
          }
        }

        if [csv_extra][xml_string] {
          # For some reason, these have a "-" after each tag-closing ">" which creates a bunch of noise
          # Replace directory separators so they don't confuse the parser as an escape character
          # the replace syntax is odd because backslashes
          mutate {
            gsub => [
              "[csv_extra][xml_string]", ">- *", ">",
              "[event_fulldata]", "[\\]", "/"
            ]
          }

          xml {
            source => "[csv_extra][xml_string]"
            target => "xmldata"
            force_array => false
          }

          # break out eventdata kv pairs to named fields
          # sometimes, though, this data is unnamed
          # this logic looks odd because if there is only one element, it doesn't go into an array.
          # this is due of the force_array setting in the xml load, which is needed for other reasons in this data type
          if [xmldata][EventData][Data] and !([xmldata][EventData][Data][0][Name] or [xmldata][EventData][Data][Name]) {
            mutate {
              convert => { "[xmldata][EventData][Data]" => "string" }
              rename => {
                "[xmldata][EventData][Data]" => "[xmldata][EventData][string]"
              }
            }

          } else if [xmldata][EventData][Data] {
            ruby {
              path => "/usr/local/sof-elk/supporting-scripts/split_kv_to_fields.rb"
              script_params => {
                "source_field" => "[xmldata][EventData][Data]"
                "destination_field" => "[xmldata][event_data]"
                "key_field" => "Name"
                "val_field" => "content"
              }
            }
          }

          # use this timestamp as a more original source of truth
          if [xmldata][System][TimeCreated][SystemTime] {
            date {
              match => [ "[xmldata][System][TimeCreated][SystemTime]", "ISO8601" ]
            }
          }

          # sometimes, this is in the xml as '<EventID Qualifiers="16384">7040</EventID>', others as '<EventID>7040</EventID>'  handle each case
          if [xmldata][System][EventID][content] {
            mutate {
              rename => {
                "[xmldata][System][EventID][Qualifiers]" => "[winlog][event_id_qualifiers]"
              }
            }
          }

          mutate {
            rename => {
              "[xmldata][event_data][Action Name]" => "[event][type]"
              "[xmldata][event_data][CommandLine]" => "[process][command_line]"
              "[xmldata][event_data][Company]" => "[winlog][event_data][Company]"
              "[xmldata][event_data][CurrentDirectory]" => "[process][working_directory]"
              "[xmldata][event_data][Description]" => "[winlog][event_data][Description]"
              "[xmldata][event_data][FileVersion]" => "[process][pe][file_version]"
              "[xmldata][event_data][Image]" => "[process][executable]"
              "[xmldata][event_data][LogonGuid]" => "[winlog][event_data][LogonGuid]"
              "[xmldata][event_data][LogonId]" => "[winlog][event_data][LogonId]"
              "[xmldata][event_data][OriginalFileName]" => "[process][pe][original_file_name]"
              "[xmldata][event_data][ParentCommandLine]" => "[process][parent][command_line]"
              "[xmldata][event_data][ParentImage]" => "[process][parent][executable]"
              "[xmldata][event_data][ParentProcessGuid]" => "[process][parent][entity_id]"
              "[xmldata][event_data][ParentProcessId]" => "[process][parent][pid]"
              "[xmldata][event_data][ParentUser]" => "[winlog][event_data][ParentUser]"
              "[xmldata][event_data][ProcessGuid]" => "[process][entity_id]"
              "[xmldata][event_data][Product]" => "[winlog][event_data][Product]"
              "[xmldata][event_data][QueryName]" => "[dns][question][name]"
              "[xmldata][event_data][QueryResults]" => "[@metadata][QueryResults]"
              "[xmldata][event_data][RuleName]" => "[rule][name]"
              "[xmldata][event_data][Signature]" => "[winlog][event_data][Signature]"
              "[xmldata][event_data][SignatureStatus]" => "[winlog][event_data][SignatureStatus]"
              "[xmldata][event_data][Signed]" => "[winlog][event_data][Signed]"
              "[xmldata][event_data][TargetFilename]" => "[winlog][event_data][TargetFilename]"
              "[xmldata][event_data][TerminalSessionId]" => "[winlog][event_data][TerminalSessionId]"
              "[xmldata][event_data][Threat Name]" => "[threat][software][name]"
              "[xmldata][event_data][User]" => "[@metadata][User]"
              "[xmldata][event_data][UtcTime]" => "[@metadata][UtcTime]"
              "[xmldata][System][Channel]" => "[winlog][channel]"
              "[xmldata][System][Execution][ProcessID]" => "[process][pid]"
              "[xmldata][System][Execution][ThreadID]" => "[winlog][process][thread][id]"
              "[xmldata][System][EventRecordID]" => "[winlog][record_id]"
            }
          }

          # if this exists, use it over the syslog datetime since it's in UTC
          if [@metadata][UtcTime] {
            date {
              match => [ "[@metadata][UtcTime]", "ISO8601" ]
            }
          }

          # this is in SYSTEM LOCAL TIME
          if [xmldata][System][TimeCreated][SystemTime] {
            date {
              match => [ "[xmldata][System][TimeCreated][SystemTime]", "ISO8601" ]
              target => "[winlog][event_data][CreationUtcTime]"
            }
          }

          # Split these into their individual hash type fields
          if [xmldata][event_data][Hashes] {
            kv {
              source => "[xmldata][event_data][Hashes]"
              target => "[@metadata][kvhashes]"
            }
            if [@metadata][kvhashes] {
              mutate {
                rename => {
                  "[@metadata][kvhashes][MD5]" => "[process][hash][md5]"
                  "[@metadata][kvhashes][SHA256]" => "[process][hash][sha256]"
                  "[@metadata][kvhashes][IMPHASH]" => "[process][pe][imphash]"
                }
              }

              if [process][hash][md5] { mutate { merge => { "[related][hash]" => "[process][hash][md5]" } } }
              if [process][hash][sha256] { mutate { merge => { "[related][hash]" => "[process][hash][sha256]" } } }
              if [process][pe][imphash] { mutate { merge => { "[related][hash]" => "[process][pe][imphash]" } } }
            }
          }

          if [@metadata][User] {
            mutate { add_tag => [ "entry" ] }
            grok {
              patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
              match => { "[@metadata][User]" => [
                "%{WINDOMAIN:[winlog][user][domain]}/%{WINUSER:[winlog][user][name]}"
                ]
              }
              tag_on_failure => [ "_grokparsefail_6601-06"]
            }
            mutate { merge => { "[related][user]" => "[winlog][user][name]" } }
          }

          # Empty rule names are "-" and sometimes there are multiple space-separated values
          if [rule][name] {
            if [rule][name] == "-" {
              mutate {
                remove_field => [ "[rule][name]" ]
              }
            } else {
              mutate {
                split => { "[rule][name]" => " " }
              }
            }
          }

          # DNS query results

          # handle answer type if present
          # this is only taking the first response type, which is admittedly not ideal
          if [@metadata][QueryResults] {
            if [@metadata][QueryResults] =~ "type:  " {
              grok {
                match => { "[@metadata][QueryResults]", [
                  "^type:  %{INT:[@metadata][dnstype]}"
                ] }
                tag_on_failure => [ "_grokparsefail_6601-07"]
              }
              if [@metadata][dnstype] {
                translate {
                  dictionary_path => "/usr/local/sof-elk/lib/dictionaries/dns_type_code2name.yaml"
                  source => "[@metadata][dnstype]"
                  target => "[dns][answers][type]"
                }
              }
              # remove all type:... substrings
              mutate {
                gsub => [ "[@metadata][QueryResults]", "type:  \d+ ", "" ]
              }
            }
            # Strip IPv6 oddity and then split on semicolon
            mutate {
              gsub => [ "[@metadata][QueryResults]", "::ffff:", "" ]
              split => { "[@metadata][QueryResults]" => ";" }
            }
            mutate {
              rename => {
                "[@metadata][QueryResults]" => "[dns][answers][data]"
              }
            }
          }
        }
      }

    # extract prefetch data fields
    } else if [csv][datasource] == "LOG" and [csv][datasourcetype] == "WinPrefetch" {
      grok {
        patterns_dir => [ "/usr/local/sof-elk/grok-patterns" ]
        match => { "[csv][desc]", [
          "Prefetch \[%{DATA:filename}\] was executed - run count %{POSINT:run_count} path: %{DATA:path} hash: %{WORD:prefetch_hash} volume: %{POSINT:volume_number} \[serial number: %{DATA:volume_serial}  device path: %{DATA:device_path}\]"
        ] }
        tag_on_failure => [ "_grokparsefail_6601-08"]
      }
    }

    # rename anything left and make copies as needed to satisfy ECS needs
    mutate {
      copy => {
        "[winlog][user][identifier]" => "[user][id]"
        "[process][pid]" => "[winlog][process][pid]"
        "[winlog][computer_name]" => "[host][hostname]"
        "[winlog][provider_name]" => "[event][provider]"
      }
      rename => {
        "[csv][volume_serial]" => "[volume][serial_number]"
        "[csv][inode]" => "[file][inode]"
      }
      replace => {
        "message" => "[csv][short]"
      }
    }

    mutate {
      convert => {
        "[log][file][device_id]" => "integer"
        "[log][file][inode]" => "integer"
        "[winlog][event_id]" => "integer"
        "[winlog][event_id_qualifiers]" => "integer"
        "[process][pid]" => "integer"
        "[winlog][process][pid]" => "integer"
        "[winlog][process][thread][id]" => "integer"
        "[winlog][recovered]" => "boolean"
        "[winlog][event_data][Signed]" => "boolean"
        "[winlog][event_data][IntegrityLevel]" => "integer"
        "[winlog][record_id]" => "integer"
        "[winlog][plaso][record_number]" => "integer"
      }

      # clean up
      remove_field => [ "csv", "xmldata", "csv_extra" ]
    }
  }
}
